{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lashike/mgmt638/blob/main/Copy_of_11_backtest_sector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz53pxPeiZhx"
      },
      "source": [
        "# Backtesting Sector Strategy\n",
        "\n",
        "### MGMT 638: Data-Driven Investments: Equity\n",
        "### Kerry Back, Rice University\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/kerryback/mgmt638/blob/main/notebooks/11-backtest_sector.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIde9HAViZh5"
      },
      "source": [
        "## Read data\n",
        "\n",
        "- Penny stocks have been eliminated\n",
        "- Data includes both large caps and small caps.  You can filter to small caps if you want.\n",
        "- Filter to your sector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zXmyKQoiZh7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://www.dropbox.com/s/lm4v48d51g64l0f/data-2023-11-29.csv?dl=1\"\n",
        "df = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkAqFu9HiZh9",
        "outputId": "c916e99b-0bda-486a-9eb8-c92361aba86f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'    \\ndf[\"rnk\"] = df.groupby(\"date\", group_keys=False).marketcap.rank(\\n    ascending=False, \\n    method=\"first\"\\n)\\ndf = df[(df.rnk>1000) & (df.rnk<=3000)]\\ndf = df.drop(columns=[\"rnk\"])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# uncomment and execute the following to filter to small caps\n",
        "\n",
        "\"\"\"\n",
        "df[\"rnk\"] = df.groupby(\"date\", group_keys=False).marketcap.rank(\n",
        "    ascending=False,\n",
        "    method=\"first\"\n",
        ")\n",
        "df = df[(df.rnk>1000) & (df.rnk<=3000)]\n",
        "df = df.drop(columns=[\"rnk\"])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLTtP66IiZh_"
      },
      "source": [
        "### Select a sector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PouoASdHiZh_"
      },
      "outputs": [],
      "source": [
        "sector = \"Energy\"\n",
        "df = df[df.sector==sector]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmwMrVMniZiA"
      },
      "source": [
        "## Define model and target\n",
        "\n",
        "- Current code uses max_depth=4 and n_estimators=200\n",
        "- Two possible targets: return in excess of the median or rank of the return.\n",
        "- Comment one of them out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKjtvRzgiZiB",
        "outputId": "ba16bd1d-32f5-4883-d761-cb79e4029ed6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\n# could use this instead\\n\\ndf[\"target\"] = df.groupby(\"date\", group_keys=False).ret.apply(\\n    lambda x: 100 * x.rank(pct=True)\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "forest = RandomForestRegressor(max_depth=4, n_estimators=200)\n",
        "\n",
        "\n",
        "df[\"target\"] = df.groupby(\"date\", group_keys=False).ret.apply(\n",
        "    lambda x: 100 * (x-x.median())\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "# could use this instead\n",
        "\n",
        "df[\"target\"] = df.groupby(\"date\", group_keys=False).ret.apply(\n",
        "    lambda x: 100 * x.rank(pct=True)\n",
        ")\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXJKclVhiZiD"
      },
      "source": [
        "## Define predictors (features)\n",
        "\n",
        "- Leaving out interactions with market volatility, because they didn't seem to make much difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF49bv3ZiZiD"
      },
      "outputs": [],
      "source": [
        "features = [\n",
        "    \"marketcap\",\n",
        "    \"pb\",\n",
        "    \"mom\",\n",
        "    \"volume\",\n",
        "    \"volatility\",\n",
        "    \"roe\",\n",
        "    \"accruals\",\n",
        "    \"agr\"\n",
        "]\n",
        "features.sort()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj4GdLZ9iZiE"
      },
      "source": [
        "## Define training dates and training windows\n",
        "\n",
        "- Start training once we have three years of data.\n",
        "- Specify num_years_for_training $\\ge 3$ as the number of years of past data to train on in each iteration of the backtesting loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zWwTMMfiZiF"
      },
      "outputs": [],
      "source": [
        "num_years_for_training = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTrmW47niZiF"
      },
      "outputs": [],
      "source": [
        "dates = list(df.date.unique())\n",
        "dates.sort()\n",
        "train_dates = dates[156::52]  # once per year starting after three years\n",
        "\n",
        "past_dates = {}               # dates on which to train for each training date\n",
        "future_dates = {}             # dates for which to predict for each training date\n",
        "for date in train_dates:\n",
        "    start_index = dates.index(date) - 52*num_years_for_training\n",
        "    start_index = start_index if start_index >= 0 else 0\n",
        "    past_dates[date] = dates[start_index:dates.index(date)]\n",
        "    if date < train_dates[-1]:\n",
        "        future_dates[date] = dates[dates.index(date):(dates.index(date)+52)]\n",
        "    else:\n",
        "        future_dates[date] = dates[dates.index(date):]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDzlhanqiZiG"
      },
      "source": [
        "## Run the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXo4ObFMiZiG"
      },
      "outputs": [],
      "source": [
        "new_data = None\n",
        "for date in train_dates:\n",
        "    past = past_dates[date]\n",
        "    past = df[df.date.isin(past)]\n",
        "    future = future_dates[date]\n",
        "    future = df[df.date.isin(future)]\n",
        "    forest.fit(X=past[features], y=past.target)\n",
        "    predictions = forest.predict(X=future[features])\n",
        "    predictions = pd.DataFrame(predictions)\n",
        "    predictions.columns = [\"predict\"]\n",
        "    for col in [\"ticker\", \"date\"]:\n",
        "        predictions[col] = future[col].to_list()\n",
        "    new_data = pd.concat((new_data, predictions))\n",
        "\n",
        "df = df.merge(new_data, on=[\"ticker\", \"date\"], how=\"inner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Rshn1EiZiG"
      },
      "source": [
        "## Calculate portfolio returns\n",
        "\n",
        "- Specify how many stocks you want to hold in each (long or short) portfolio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55v3oTzFiZiH"
      },
      "outputs": [],
      "source": [
        "numstocks = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnsip--IiZiH"
      },
      "outputs": [],
      "source": [
        "df[\"rnk_long\"] = df.groupby(\"date\", group_keys=False).predict.rank(\n",
        "    ascending=False,\n",
        "    method=\"first\"\n",
        ")\n",
        "df[\"rnk_short\"] = df.groupby(\"date\", group_keys=False).predict.rank(\n",
        "    ascending=True,\n",
        "    method=\"first\"\n",
        ")\n",
        "\n",
        "\n",
        "longs = df[df.rnk_long<=numstocks]\n",
        "shorts = df[df.rnk_short<=numstocks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOGOrotdiZiH"
      },
      "outputs": [],
      "source": [
        "long_ret = longs.groupby(\"date\").ret.mean()\n",
        "short_ret = shorts.groupby(\"date\").ret.mean()\n",
        "print(f\"mean annualized long return is {52*long_ret.mean():.2%}\")\n",
        "print(f\"mean annualized short return is {52*short_ret.mean():.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F15hSV88iZiI"
      },
      "source": [
        "## Evaluate long returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI-LGDx1iZiI"
      },
      "source": [
        "### Get weekly factors and risk-free rate\n",
        "\n",
        "- There is some weekly data on French's website, but not everything we want is available weekly.\n",
        "- So, we will get daily data and compound to weekly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ-9KKK3iZiI"
      },
      "outputs": [],
      "source": [
        "from pandas_datareader import DataReader as pdr\n",
        "\n",
        "famafrench = pdr(\"F-F_Research_Data_5_Factors_2x3_daily\", \"famafrench\", start=2010)[0] / 100\n",
        "famafrench.index.name = \"date\"\n",
        "famafrench = famafrench.reset_index()\n",
        "famafrench[\"year\"] = famafrench.date.apply(lambda x: x.isocalendar()[0])\n",
        "famafrench[\"week\"] = famafrench.date.apply(lambda x: x.isocalendar()[1])\n",
        "\n",
        "ff = None\n",
        "for col in [\"Mkt-RF\", \"SMB\", \"HML\", \"CMA\", \"RMW\", \"RF\"]:\n",
        "    ser = famafrench.groupby([\"year\", \"week\"], group_keys=True)[col].apply(\n",
        "        lambda x: (1+x).prod() - 1\n",
        "    )\n",
        "    ser.name = col\n",
        "    ff = pd.concat((ff, ser), axis=1)\n",
        "ff[\"date\"] = famafrench.groupby([\"year\", \"week\"], group_keys=True).date.last()\n",
        "ff = ff.reset_index(drop=True)\n",
        "ff = ff.set_index(\"date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlcEWKdUiZiJ"
      },
      "outputs": [],
      "source": [
        "mom = pdr(\"F-F_Momentum_Factor_daily\", \"famafrench\", start=2010)[0]/100\n",
        "mom.index.name = \"date\"\n",
        "mom.columns = [\"UMD\"]\n",
        "mom = mom.reset_index()\n",
        "mom[\"year\"] = mom.date.apply(lambda x: x.isocalendar()[0])\n",
        "mom[\"week\"] = mom.date.apply(lambda x: x.isocalendar()[1])\n",
        "\n",
        "umd = mom.groupby([\"year\", \"week\"], group_keys=True).UMD.apply(\n",
        "    lambda x: (1+x).prod() - 1\n",
        ")\n",
        "umd = pd.DataFrame(umd)\n",
        "umd[\"date\"] = mom.groupby([\"year\", \"week\"], group_keys=True).date.last()\n",
        "umd = umd.reset_index(drop=True)\n",
        "umd = umd.set_index(\"date\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7FXrT9riZiJ"
      },
      "source": [
        "### Combine factors and long returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKWxMzoqiZiJ"
      },
      "outputs": [],
      "source": [
        "long_ret.name = \"ret\"\n",
        "long_ret.index = pd.to_datetime(long_ret.index)\n",
        "data = pd.concat((ff, umd, long_ret), axis=1).dropna()\n",
        "data.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnT9sUOkiZiK"
      },
      "source": [
        "### Sharpe ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU4jAJ3CiZiK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "sharpe = np.sqrt(52) * (data.ret - data.RF).mean() / data.ret.std()\n",
        "print(f\"annualized Sharpe ratio is {sharpe:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Eo3CTmiZiK"
      },
      "source": [
        "### Market alpha and information ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-eSVvhviZiK"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "data[\"ret_rf\"] = data.ret - data.RF\n",
        "data[\"mkt_rf\"] = data[\"Mkt-RF\"]\n",
        "result = smf.ols(\"ret_rf ~ mkt_rf\", data).fit()\n",
        "\n",
        "alpha = 52*result.params[\"Intercept\"]\n",
        "resid_stdev = np.sqrt(52 * result.mse_resid)\n",
        "info_ratio = alpha / resid_stdev\n",
        "\n",
        "print(f\"annualized alpha is {alpha:.2%}\")\n",
        "print(f\"annualized information ratio is {info_ratio:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhfgI9yxiZiL"
      },
      "source": [
        "### Attribution analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3BIl3Z8iZiL"
      },
      "outputs": [],
      "source": [
        "result = smf.ols(\"ret_rf ~ mkt_rf + SMB + HML + CMA + RMW + UMD\", data).fit()\n",
        "result.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXb7mhffiZiL"
      },
      "source": [
        "## Analyze fitted model on most recent data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVOh9o9ViZiM"
      },
      "source": [
        "### Get most recent data from backtest data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxUb5zJViZiM"
      },
      "outputs": [],
      "source": [
        "present = future[future.date==future.date.max()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLXXIkOziZiM"
      },
      "source": [
        "### Visualize distributions of characteristics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn710WpxiZiM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "sns.pairplot(present[features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zkgdFmziZiN"
      },
      "source": [
        "### Calculate medians to use as base values for characteristics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAT6qcZ0iZiN"
      },
      "outputs": [],
      "source": [
        "present = future[future.date==future.date.max()]\n",
        "medians = present[features].median()\n",
        "medians = pd.DataFrame(medians).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD7_-wBViZiN"
      },
      "source": [
        "### Define plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwscC9a7iZiW"
      },
      "outputs": [],
      "source": [
        "def predict1(char):\n",
        "    data = medians.copy()\n",
        "    grid = np.linspace(\n",
        "        present[char].quantile(0.01),\n",
        "        present[char].quantile(0.99),\n",
        "        100\n",
        "    )\n",
        "    predictions = []\n",
        "    for x in grid:\n",
        "        data[char] = x\n",
        "        prediction = forest.predict(X=data).item()\n",
        "        predictions.append(prediction)\n",
        "    return grid, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGeWNMO-iZiW"
      },
      "outputs": [],
      "source": [
        "def predict2(char1, char2):\n",
        "    data = medians.copy()\n",
        "    grid1 = np.linspace(\n",
        "        present[char1].quantile(0.01),\n",
        "        present[char1].quantile(0.99),\n",
        "        20\n",
        "    )\n",
        "    grid2 = np.linspace(\n",
        "        present[char2].quantile(0.01),\n",
        "        present[char2].quantile(0.99),\n",
        "        20\n",
        "    )\n",
        "    grid1, grid2 = np.meshgrid(grid1, grid2)\n",
        "    predictions = np.empty(grid1.shape)\n",
        "    for i in range(20):\n",
        "        for j in range(20):\n",
        "            data[char1] = grid1[i, j]\n",
        "            data[char2] = grid2[i, j]\n",
        "            predictions[i, j] = forest.predict(data)\n",
        "    return grid1, grid2, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Enz0ZQOiZiW"
      },
      "source": [
        "### Feature importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFKEbQNAiZiW"
      },
      "outputs": [],
      "source": [
        "importances = pd.Series(forest.feature_importances_, index=features)\n",
        "importances.sort_values(ascending=False).round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3udCa0wdiZiX"
      },
      "source": [
        "### Vary one characteristic at a time and plot\n",
        "\n",
        "- Specify which characteristic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og8oL4Z-iZiX"
      },
      "outputs": [],
      "source": [
        "char = \"volatility\"\n",
        "\n",
        "grid, predictions = predict1(char)\n",
        "plt.plot(grid, predictions)\n",
        "plt.xlabel(char,  fontdict={\"size\": 14})\n",
        "plt.ylabel(\"prediction\",  fontdict={\"size\": 14})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHHTdwxSiZiY"
      },
      "source": [
        "### Vary two characteristics at a time and plot\n",
        "\n",
        "- Specify which characteristics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YATd4RuIiZiY"
      },
      "outputs": [],
      "source": [
        "char1 = \"marketcap\"\n",
        "char2 = \"volatility\"\n",
        "\n",
        "grid1, grid2, predictions = predict2(char1, char2)\n",
        "contour = plt.contourf(grid1, grid2, predictions, 20, cmap=\"viridis\")\n",
        "cbar = plt.colorbar(contour)\n",
        "plt.xlabel(char1, fontdict={\"size\": 14})\n",
        "plt.ylabel(char2, fontdict={\"size\": 14})\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}