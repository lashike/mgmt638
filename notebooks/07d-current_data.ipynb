{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/kerryback/mgmt638/blob/main/notebooks/07d-current_data_ver2.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Current Data\n",
    "\n",
    "### MGMT 638: Data-Driven Investments: Equity\n",
    "### Kerry Back, Rice University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pymssql\n",
    "server = 'fs.rice.edu'\n",
    "database = 'stocks'\n",
    "username = 'stocks'\n",
    "password = '6LAZH1'\n",
    "string = \"mssql+pymssql://\" + username + \":\" + password + \"@\" + server + \"/\" + database \n",
    "conn = create_engine(string).connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculate financial ratios and growth rates\n",
    "\n",
    "Data from SF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sf1 = pd.read_sql(\n",
    "    \"\"\" \n",
    "    select ticker, datekey, lastupdated, netinc, ncfo, equity, assets \n",
    "    from sf1\n",
    "    where dimension='ARQ' and datekey>='2021-01-01' and equity>0 and assets>0\n",
    "    order by ticker, datekey\n",
    "    \"\"\",\n",
    "    conn,\n",
    "    parse_dates=[\"datekey\"]\n",
    ")\n",
    "sf1 = sf1.groupby([\"ticker\", \"datekey\", \"lastupdated\"]).last()\n",
    "sf1 = sf1.droplevel(\"lastupdated\")\n",
    "sf1 = sf1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for col in [\"netinc\", \"ncfo\"]:\n",
    "    sf1[col] = sf1.groupby(\"ticker\", group_keys=False)[col].apply(\n",
    "        lambda x: x.rolling(4).sum()\n",
    "    )\n",
    "for col in [\"equity\", \"assets\"]:\n",
    "    sf1[col] = sf1.groupby(\"ticker\", group_keys=False)[col].apply(\n",
    "        lambda x: x.rolling(4).mean()\n",
    "    )\n",
    "sf1[\"roe\"] = sf1.netinc / sf1.equity\n",
    "sf1[\"accruals\"] = (sf1.netinc - sf1.ncfo) / sf1.equity\n",
    "sf1[\"agr\"] = sf1.groupby(\"ticker\", group_keys=False)[\"assets\"].pct_change()\n",
    "sf1 = sf1[[\"ticker\", \"datekey\", \"roe\", \"accruals\", \"agr\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Returns, volume, momentum, volatility\n",
    "\n",
    "Data from sep_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sep_weekly = pd.read_sql(\n",
    "    \"\"\" \n",
    "    select ticker, date, volume, closeadj, closeunadj, lastupdated \n",
    "    from sep_weekly \n",
    "    where date>='2022-01-01'\n",
    "    order by ticker, date, lastupdated    \n",
    "    \"\"\",\n",
    "    conn,\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "sep_weekly = sep_weekly.groupby([\"ticker\", \"date\", \"lastupdated\"]).last()\n",
    "sep_weekly = sep_weekly.droplevel(\"lastupdated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sep_weekly[\"ret\"] = sep_weekly.groupby(\"ticker\", group_keys=False).closeadj.pct_change()\n",
    "sep_weekly[\"annual\"] = sep_weekly.groupby(\"ticker\", group_keys=False).closeadj.pct_change(52)\n",
    "sep_weekly[\"monthly\"] = sep_weekly.groupby(\"ticker\", group_keys=False).closeadj.pct_change(4)\n",
    "sep_weekly[\"mom\"] = sep_weekly.groupby(\"ticker\", group_keys=False).apply(\n",
    "    lambda d: (1+d.annual)/(1+d.monthly) - 1\n",
    ")\n",
    "sep_weekly[\"volatility\"] = sep_weekly.groupby(\"ticker\", group_keys=False).ret.apply(\n",
    "    lambda x: x.rolling(26).std()\n",
    ")\n",
    "sep_weekly = sep_weekly[[\"mom\", \"volume\", \"volatility\", \"closeunadj\"]]\n",
    "sep_weekly = sep_weekly.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Get marketcap and pb\n",
    "\n",
    "Data from weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "weekly = pd.read_sql(\n",
    "    \"\"\" \n",
    "    select ticker, date, marketcap, pb, lastupdated\n",
    "    from weekly\n",
    "    where date>='2022-01-01' and marketcap>0 and pb>0\n",
    "    order by ticker, date, lastupdated\n",
    "    \"\"\",\n",
    "    conn,\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "weekly = weekly.groupby([\"ticker\", \"date\", \"lastupdated\"]).last()\n",
    "weekly = weekly.droplevel(\"lastupdated\")\n",
    "weekly = weekly.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = weekly.merge(sep_weekly, on=[\"ticker\", \"date\"], how=\"inner\")\n",
    "df[\"year\"] = df.date.apply(lambda x: x.isocalendar()[0])\n",
    "df[\"week\"] = df.date.apply(lambda x: x.isocalendar()[1])\n",
    "sf1[\"year\"] = sf1.datekey.apply(lambda x: x.isocalendar()[0])\n",
    "sf1[\"week\"] = sf1.datekey.apply(lambda x: x.isocalendar()[1])\n",
    "df = df.merge(sf1, on=[\"ticker\", \"year\", \"week\"], how=\"left\")\n",
    "df = df.drop(columns=[\"year\", \"week\", \"datekey\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fill ratios and growth rates forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for col in [\"roe\", \"accruals\", \"agr\"]:\n",
    "    df[col] = df.groupby(\"ticker\", group_keys=False)[col].apply(\n",
    "        lambda x: x.ffill()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Add sector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tickers = pd.read_sql(\n",
    "    \"\"\" \n",
    "    select ticker, sector from tickers\n",
    "    \"\"\",\n",
    "    conn\n",
    ")\n",
    "df = df.merge(tickers, on=\"ticker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate market volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "price = yf.download(\"SPY\", start=\"2023-01-01\")[\"Adj Close\"]\n",
    "ret = price.pct_change()\n",
    "vol = np.sqrt(252)*ret.rolling(21).std()\n",
    "vol.name = \"mktvol\"\n",
    "vol.index.name = \"date\"\n",
    "vol = pd.DataFrame(vol).reset_index()\n",
    "vol[\"year\"] = vol.date.apply(lambda x: x.isocalendar()[0])\n",
    "vol[\"week\"] = vol.date.apply(lambda x: x.isocalendar()[1])\n",
    "vol = vol.groupby([\"year\", \"week\"]).last()\n",
    "vol = vol[[\"date\", \"mktvol\"]].set_index(\"date\")\n",
    "vol[\"mktvol\"] = vol.mktvol.shift()\n",
    "vol = vol.dropna()\n",
    "df = df.merge(vol, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filter to today's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df.date==df.date.max()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filter to small caps and exclude penny stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df.closeunadj>5]\n",
    "df = df.dropna()\n",
    "df[\"rnk\"] = df.marketcap.rank(\n",
    "    ascending=False, \n",
    "    method=\"first\"\n",
    ")\n",
    "df = df[(df.rnk>1000) & (df.rnk<=3000)]\n",
    "df = df.drop(columns=[\"closeunadj\", \"rnk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"data-current-2023-11-13.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
